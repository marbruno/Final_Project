---
title: "Using Predictive Analytics to Understand the Impacts of COVID-19 on Unemployment in the U.S."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

### Introduction
While the COVID-19 pandemic started as a public health crisis, households in the U.S. and other countries experienced the pandemic as a accumulated housing, labor, and financial crises as well. Given the dramatic and often unpredictable impact of the pandemic on the labor force, determining how to predict the economic impact of future, similar events is of interest. Being able to predict whether an individual experiences unemployment as a result of the pandemic, for example, may be useful in planning targeted future interventions during such unusual times.


### About This Project
The primary contribution of our project is to generate a model to predict unemployment using data before and during the pandemic (2019 and 2020).

To better understand our modeling data, we develop several data visualizations using data from the Current Population Survey (CPS) ASEC supplements (2019-2021) to see how unemployment and employment varies by age group and state. We then identify variables used to include as predictors in our predictive model and use Principal Components Analysis (PCA) to generate uncorrelated components to capture a portion of the variance of the original predictors. Using both our original predictors and the resulting PCs, we generate a random forest model and a logistic regression model trained on ASEC data from 2019 and 2020 to predict whether an individual was employed. We then implement the best model (selected by identifying the model with the highest area under the ROC curve) on data from 2021 to test its accuracy in predicting whether an individual was employed in the second year of the pandemic. We also test the model on a subset of the population (immigrants) to see if the model, which was developed with both immigrants and non-immigrants, is generalizable to only immigrants. With a model like this, we can predict the employment status for future individuals for whom we have data on their characteristics (e.g., income, state, type of health insurance) so that when a future pandemic or natural disaster happens, policymakers will be better equipped to understand who to target support towards. This model can therefore support an informed approach to outreach, so that governments or social service agencies can focus their resources with targeted outreach to the individuals that are predicted to be unemployed after the start of the pandemic.

### Data Sources
For this analysis, we employ the [Annual Social and Economic Supplemental (ASEC)](https://www.census.gov/programs-surveys/saipe/guidance/model-input-data/cpsasec.html) of the Current Population Survey (CPS), also known as the March Supplemental survey. The CPS ASEC survey is conducted annually, mostly in March, during a one month window of data collection. The survey asks participants a series of questions that are similar to the CPS survey, but also asks about topics related to income, earnings, poverty, and the foreign-born population.^[https://www.census.gov/topics/population/foreign-born/guidance/cps-guidance/cps-vs-asec.html] Our project uses the March Supplemental survey results from 2019, 2020, and 2021. We accessed these data through Integrated Public Use Microdata Series (IPUMS) at the University of Minnesota.^[Sarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren and Michael Westberry. Integrated Public Use Microdata Series, Current Population Survey: Version 9.0 [dataset]. Minneapolis, MN: IPUMS, 2021. https://doi.org/10.18128/D030.V9.0]

To view further details of the project, visit the project's public GitHub repository [here](https://github.com/marbruno/Final_Project).

Below, we load in our libraries.
``` {r add-libraries}
# Add libraries
library(formattable)
library(ggalt)
library(ggplot2)
library(haven)
library(ipumsr)
library(labelled)
library(patchwork)
library(sf)
library(sjlabelled)
library(srvyr)
library(tidyverse)
library(tigris)

```

Next, we read in the ASEC data, recoding missing values as NAs, recoding variables as dummies, dropping extraneous variables, filtering out people in the armed forces, and selecting only adults aged 18 to 65 (inclusive). We also create two new variables: `employed`, which indicates whether a respondent is unemployed or employed and is NA for anyone outside of the labor force; and `immigrant`, which is a dummy variable coded as 1 for people born in the U.S. and 0 for people born in the U.S. outlying territories (e.g., PR, Guam), naturalized citiens, non-citizens, and people born abroad of American parents. Finally, we create the data tables that we will use throughout our analysis.
```{r loading-data, results="hide"}
ddi <- read_ipums_ddi("data/cps_00004.xml")
data <- read_ipums_micro(ddi) %>%
  janitor::clean_names()

cps_svy <- data %>%
  # Filter  out people in armed forces
  filter(empstat != 1) %>%
  filter(age >= 18 & age < 66) %>%
  # Filter out minors; leave only adults
  mutate(employed = case_when(
    empstat == 10 | empstat == 12 ~ 1,
    empstat == 21 | empstat == 22 ~ 0,
    TRUE ~ NA_real_
  )) %>%
  # Recode variables as dummies (0,1)
  mutate_at(vars(offpov, himcarenw, caidnw, anycovly, prvtcovnw, grpcovnw, mrkcovnw, mrkscovnw, mrkucovnw, inhcovnw, sex), list(~ case_when(
    . == 2 ~ 1,
    . == 1 ~ 0,
    TRUE ~ NA_real_
  ))) %>%
  # Turn values coded as missing into NAs
  mutate(unitsstr = case_when(
    unitsstr == 00 ~ NA_real_,
    TRUE ~ as.numeric(unitsstr)
  )) %>%
  mutate(region = case_when(
    region == 97 ~ NA_real_,
    TRUE ~ as.numeric(region)
  )) %>%
  mutate(metro = case_when(
    metro == 0 | metro >= 4 ~ NA_real_,
    TRUE ~ as.numeric(metro)
  )) %>%
  mutate(metarea = case_when(
    metarea >= 9997 ~ NA_real_,
    TRUE ~ as.numeric(metarea)
  )) %>%
  mutate(metfips = case_when(
    metfips >= 99998 ~ NA_real_,
    TRUE ~ as.numeric(metfips)
  )) %>%
  mutate(hhincome = case_when(
    hhincome == 9999999 ~ NA_real_,
    TRUE ~ as.numeric(hhincome)
  )) %>%
  mutate_at(vars(county, empstat, labforce, disabwrk), list(~ case_when(
    . == 0 ~ NA_real_,
    TRUE ~ as.numeric(.)
  ))) %>%
    mutate(educ = case_when(
    educ == 001 | educ == 999 ~ NA_real_,
    TRUE ~ as.numeric(educ)
  )) %>%
  mutate(ftotval = case_when(
    ftotval == 9999999999 ~ NA_real_,
    TRUE ~ as.numeric(ftotval)
  )) %>%
  mutate(inctot = case_when(
    inctot == 999999999 ~ NA_real_,
    TRUE ~ as.numeric(inctot)
  )) %>%
  mutate(incwelfr = case_when(
    incwelfr == 999999 ~ NA_real_,
    TRUE ~ incwelfr
  )) %>%
  mutate(incunemp = case_when(
    incunemp == 999999 ~ NA_real_,
    TRUE ~ as.numeric(incunemp)
  )) %>%
  mutate(ctccrd = case_when(
    ctccrd == 999999 ~ NA_real_,
    TRUE ~ as.numeric(ctccrd)
  )) %>%
  mutate(eitcred = case_when(
    eitcred == 9999 ~ NA_real_,
    TRUE ~ as.numeric(eitcred)
  )) %>%
  mutate(immigrant = case_when(
    citizen == 1 ~ 1,
    citizen >= 2 & citizen <= 5 ~ 0,
    TRUE ~ NA_real_
  )) %>%
    mutate(wksunem1 = case_when(
    wksunem1 == 99 ~ 0,
    TRUE ~ as.numeric(wksunem1)
  )) %>%
    mutate(wksunem2 = case_when(
    wksunem2 == 9 ~ 0,
    TRUE ~ as.numeric(wksunem2)
  )) %>%
  # Drop multi-state fips codes
  filter(statefip <= 56) %>%
  # Drop quality flags and variables we don't need
  select(-starts_with("q"), -asecwt, -month, -asecflag, -statecensus, -asecwth, -pernum, -cpsidp)

# Create a weighted survey object using COVID-19 ASEC weight
asec_allyears <- as_survey_design(cps_svy, weights = asecwtcvd)

# Create tibbles by year and tibbles with only immigrant population
asec2019 <- asec_allyears %>%
  filter(year == 2019)

asec2020 <- asec_allyears %>%
  filter(year == 2020)

asec2021 <- asec_allyears %>%
  filter(year == 2021)

asec_allyears_imm <- asec_allyears %>%
  filter(immigrant == 1)

asec2019_imm <- asec2019 %>%
  filter(immigrant == 1)

asec2020_imm <- asec2020 %>%
  filter(immigrant == 1)

asec2021_imm <- asec2021 %>%
  filter(immigrant == 1)

```

### Visualizing Employment
The first cases of COVID-19 appeared in the U.S. in early January, but it took several months for employees to feel the full effect of the pandemic on employment. As cases rose, businesses closed. Below we see the beginning of the impact that COVID-19 will come to have on the U.S. labor market. In March of 2020, there was a 1.5 percentage point increase in the unemployment rate from the year prior. A year later, we can see the sustained impact of the pandemic on the labor market and the repercussions from the Alpha variant, which peaked in January of 2021. In March 2021, the unemployment rate was about 2.5 percentage points higher than it was at the same time in 2019 (or about 1 percentage point higher than a year prior).

```{r unempoyed-viz, fig.align = "center"}
# % Unemployed, employed, out of labor force in each
asec_allyears %>%
  drop_na(employed) %>%
  select(year, employed) %>%
  group_by(year, employed) %>%
  summarise(pct = survey_prop()*100) %>%
  ggplot(aes(x=year, y=pct, fill=factor(employed))) +
  geom_col(position="dodge") +
  labs(
    title = "Percentage of Unemployed and Employed Out of the Labor Force",
    subtitle = "The unemployment rate grew by 1.5 and 1 percentage points \n between 2019 and 2020 and 2020 and 2021, respectively",
    caption = "Source: IPUMS Data",
    x = "Year",
    y = "%",
    fill = "Status",
  ) +
  geom_text(aes(label = round(pct,2)),
                position = position_dodge(width = 1),
                color="black",vjust = -.2) +
  scale_fill_hue(labels = c("Unemployed", "Employed")) +
  theme(legend.title = element_blank()) +
  theme_minimal()

```

Overall, the share of employed people by age group remains relatively constant between 2019 and 2021. The most common age bracket among the unemployed was 18-24 year olds in all years included in our research (2019, 2020, and 2021), but their share of the unemployed decrease between 2019 and 2021.

```{r employ-age,  fig.align = "center"}
# How does age differ between the employed and unemployed populations?
# Employed
employed_graph <- asec_allyears %>%
   mutate(
    # Create categories
    age_group = case_when(
      age > 17 & age <= 25 ~ "18-24",
      age > 24 & age <= 31 ~ "25-31",
      age > 21 & age <= 38 ~ "32-38",
      age > 38 & age <= 45 ~ "39-45",
      age > 45 & age <= 52 ~ "46-52",
      age > 52 & age <= 59 ~ "53-59",
      age > 59 & age <= 65 ~ "60-65"
    )) %>%
  drop_na(employed) %>%
  filter(employed == 1) %>%
  select(year, employed, age, age_group) %>%
  group_by(year, age_group) %>%
  summarise(cnt = n()) %>%
  mutate(freq = percent(cnt / sum(cnt))) %>%
  ggplot(aes(x=age_group, y=freq, fill=factor(year))) +
  geom_col(position="dodge", show.legend=FALSE) +
  geom_text(aes(age_group, label = freq), position = position_dodge(width = 1), hjust = -.25) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent, limits = c(0,.3)) +
  labs(
    title = "Age Composition of Employed Population by Year",
    subtitle = "The age composition of the employed population \n stayed relatiely consistent from 2019 to 2021, but this is \n not true of the unemployed population",
    x = "Age Group",
    y = "",
    fill = "Year"
  ) +
  theme_minimal()


# Unemployed
unemployed_graph <- asec_allyears %>%
   mutate(
    # Create categories (18-24; 24-30; 31-37; 38-44, 45-51, 52-58, 58-64)
    age_group = case_when(
      age > 17 & age <= 25 ~ "18-24",
      age > 24 & age <= 31 ~ "25-31",
      age > 21 & age <= 38 ~ "32-38",
      age > 38 & age <= 45 ~ "39-45",
      age > 45 & age <= 52 ~ "46-52",
      age > 52 & age <= 59 ~ "53-59",
      age > 59 & age <= 65 ~ "60-65"
    )) %>%
  drop_na(employed) %>%
  filter(employed == 0) %>%
  select(year, employed, age, age_group) %>%
  group_by(year, age_group) %>%
  summarise(cnt = n()) %>%
  mutate(freq = percent(cnt / sum(cnt))) %>%
  ggplot(aes(x=age_group, y=freq, fill=factor(year))) +
  geom_col(position="dodge") +
  geom_text(aes(age_group, label = freq), position = position_dodge(width = 1), hjust = -.25) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent, limits = c(0,.35)) +
  labs(
    title = "Age Composition of Unemployed Population by Year",
    subtitle = "(in % for 2019, 2020, 2021)",
    caption = "Source: IPUMS Data",
    y = "",
    fill = "Year",
  ) +
  theme(legend.title = element_blank()) +
  theme_minimal()

employed_graph + unemployed_graph

```

### Understanding Unemployment at the State Level
The majority of states saw dramatic increases in their unemployment rate after the pandemic began, though some saw only minor changes. In the data represented below, there are increases as severe as 4.5 percentage points, but many states have an increase closer to 2 percentage points. This uptick in unemployment left many to rely on unemployment assistance, which varied significantly in amount and ease-of-access between states. ***SOURCE
<br>
<br>
Notably, there are also a few states (e.g., Alaska, Maine, Kentucky) where unemployment fell slightly betweeen March 2019 and March 2020, potentially due to their more moderate approach to pandemic mitigation. ***SOURCE
``` {r dumbbell, fig.align = "center"}
# Dumbbell % unemployed by state between 2019 and 2020
status_bystate <- asec_allyears %>%
  drop_na(employed) %>%
  filter(year ==2019 | year == 2020) %>%
  select(year, employed, statefip) %>%
  group_by(year, statefip, employed) %>%
  summarise(cnt = n()) %>%
  mutate(freq = percent(cnt / sum(cnt)))

status_bystate %>%
  filter(employed == 0) %>%
  select(year, statefip, freq) %>%
  pivot_wider(names_from = year, values_from = freq) %>%
  ggplot(aes(x=`2019`, xend=`2020`, y=sjlabelled::as_label(statefip))) +
  geom_dumbbell(color="#a3c4dc",
                colour_x="#edae52",
                      colour_xend = "#9fb059",
                      size=0.75,
                      point.colour.l="#0e668b") +
  scale_x_continuous(label = percent)  +
  labs(
    title = "Unemployment Rate by State",
    subtitle = "Most states saw sizable jumps in unemployment",
    caption = "Source: IPUMS Data",
    x = "",
    y = "States",
  ) +
  theme_minimal()

```

We see significant variation in unemployment at the state level. In only March 2020, only two months after the first case hit the U.S., states were responding with stay-at-home orders and closing businesses were firing or laying off employees. One of the hardest-hit industries was leisure and hospitality, which is a core industry in Nevada and was quickly impacted when closures began.^[Aaron Klein and Ember Smith. Explaining the Economic Impact of COVID-19: Core Industries and the Hispanic Workforce. Feb. 5, 2021. https://www.brookings.edu/research/explaining-the-economic-impact-of-covid-19-core-industries-and-the-hispanic-workforce/.]


```{r, download-shapefiles, results = "hide"}
states <- tigris::states(cb = TRUE, progress_bar = FALSE) %>%
  filter(!STATEFP %in% c("78", "69", "66", "60", "72", "02", "15"))
```
### We can maybe delete the below code if it's already converted into a survey object, but it must be converted into a survey object to create the map
```{r, turning-the df-into-a-survey-object-with-weights, results = "hide"}
cps_svy_map <- cps_svy %>%
  as_survey_design(weights = asecwtcvd)
```

```{r, creating-a-map}
# filtering cps_svy_map to the desired year, grouping to state, and getting the mean unemployment
df1 <- cps_svy_map %>%
  filter(year == 2020, !is.na(employed)) %>%
   group_by(statefip) %>%
   srvyr::summarize(emp_rate = survey_mean(1 - employed)) %>%
   rename(STATEFP = statefip)

# turning the state fip variable into a character, for compatibility before joining
df2 <- rapply(df1, as.character, how = "replace") %>%
   mutate(
     STATEFP = str_pad(STATEFP, width = 2, side = c("left"), pad = "0"))

# joining the df with unemployment to the shapefile
sf1 <- left_join(states, df2, by = "STATEFP")

# mapping unemployment for contiguous US
unemp_map <- df2 %>%
   ggplot() +
   geom_sf(data = sf1, mapping = aes(fill = as.numeric(emp_rate))) +
   scale_fill_distiller(direction = 1, name = "Unemployment Rate", palette = "YlGnBu") +
   theme_void() +
   labs(title = "Unemployment in the Contiguous U.S. in March 2020",
        subtitle = "State-level unemployment varies from a low of 3% to a high of 8%")

unemp_map
```

<!-- ### Modeling -->

<!-- After having looked at our employment data, we begin to put together our models, loading in the relevant libraries and creating a tibble for our 2019 and 2020 data for the model below. -->
<!-- ```{r model-prep, results="hide"} -->
<!-- # Load modeling libraries -->
<!-- library(parsnip) -->
<!-- library(survey) -->
<!-- library(tidymodels) -->

<!-- # Create tibble with 2019 and 2020 data for model training, testing -->
<!-- # Removes variable we're trying to predict (employed) -->
<!-- asec_2019_2020 <- cps_svy %>% -->
<!--   filter(year == 2019 | year == 2020) %>% -->
<!--   filter(!is.na(employed)) -->

<!-- ``` -->

<!-- When conceptualizing the potential different models we could build to predict people's employment status, we were curious to see if principal components could be used as predictors. To avoid data leakage, it is necessary for the principal components to be constructed each time our model specifications run through new cross-validation folds. As such, principal component analysis needed to be part of our pre-processing, and we included it as the last step of our model recipe. Nevertheless, we had to construct a specific data set to use principal components as predictors since we needed to convert categorical variables into dummy variables without increasing their weight in the machine learning algorithms. -->
<!-- <br> -->
<!-- <br> -->
<!-- We also needed to recode variables that had NA values. Most of these NAs were in categorical variables and were due to the question not being asked of the entire survey population. In these instances, we created a dummy variable for whether the variable took on a value of NA. Where the NA was in a numeric variable, we recoded it to zero in reading in the data because in these cases given that not being in the question universe means that non-respondents had a value of zero for these variables (e.g., people who did not look for work in the past year equivalently looked for work zero weeks of the past year). We did not want to impute values for these observations because they were not eligible to answer the question, and so imputation may have led to impossible or improbable sets of responses (e.g., someone who was employed for the past year but is imputed to have looked for work for 5 weeks in the past year). -->
<!-- <br> -->
<!-- <br> -->
<!-- Finally, in preparing for PCA, we also recoded dummy variables as taking on values 0 and sqrt(n), where n is the original number of categories included in the categorical variable that the dummies were generated from, so that categorical variables with a greater number of categories do not receive greater weight in the PCA compared to categorical variables with fewer categories. -->

<!-- ```{r prep-data-pca-modeling, results="hide"} -->
<!-- # ------------------------------------Creating data frame for step_pca------------------------------------ -->

<!-- # Select relevant variables from data set -->
<!-- asec_pca_2019_2020 <- asec_2019_2020 %>% -->
<!--   select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include in PCA analysis -->
<!--   select(-region, -county, -metro, -metarea, -metfips) %>% # deselect all location variables other than state -->
<!--   select(-empstat, -labforce, -asecwtcvd) %>% # deselect variables that are unuseful (labforce) or redundant with employed variable -->
<!--   mutate_at(vars(race, unitsstr, citizen, hispan, -->
<!--             occ, ind, educ, classwly, -->
<!--             strechlk, spmmort, health, paidgh, whymove, statefip, employed), list(~ as.factor(.))) -->

<!-- # Turn categorical variables into separate dummies -->
<!-- asec_pca_2019_2020 <- recipe(~ ., asec_pca_2019_2020) %>% -->
<!--   step_dummy(race, unitsstr, citizen, hispan, -->
<!--              occ, ind, educ, classwly, -->
<!--              strechlk, spmmort, whymove, health, paidgh, statefip) %>% -->
<!--   prep() %>% -->
<!--   bake(asec_pca_2019_2020) -->

<!-- # Change values of dummy variables such that being in a certain category is indicated with -->
<!-- # a value of sqrt(n), where n = number of categories of the original categorical variable used to create the dummies -->
<!-- asec_pca_2019_2020 <- asec_pca_2019_2020 %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       offpov, himcarenw, caidnw, anycovly, prvtcovnw, grpcovnw, mrkcovnw, -->
<!--       mrkscovnw, inhcovnw, mrkucovnw, sex, starts_with("spmmort") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ (1/sqrt(2)), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("citizen"), starts_with("health") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(4), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("unitsstr") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(5), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("classwly") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(7), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("hispan") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(8), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("educ") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(15), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("race") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(25), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("state") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(50), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--     ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("ind") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(279), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("occ") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(633), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("classwly") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(7), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("whymove") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(20), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) %>% -->
<!--   mutate_at( -->
<!--     vars( -->
<!--       starts_with("paidgh") -->
<!--     ), -->
<!--     list( ~ case_when(. == 1 ~ 1/sqrt(3), -->
<!--                       . == 0 ~ 0, -->
<!--                       TRUE ~ NA_real_)) -->
<!--   ) -->

<!-- ``` -->

<!-- We decided to first try using principal components as predictors in a logistic regression model to predict whether someone was employed or not. We attempted to run a logistical regression model with the same predictors used to generate the PCs, but found it computationally too expensive Below, we prepare the logistic model with four folds of the training data--fewer than the standard ten folds again for the sake of computational speed. -->

<!-- ``` {r Model-1, results="hide"} -->
<!-- # Preparing data for models -->

<!-- # Set seed so that selection of training/testing data is consistent between runs -->
<!-- # of the code chunk -->
<!-- set.seed(20201020) -->

<!-- # Split into training and testing data -->
<!-- split <- initial_split(data = asec_pca_2019_2020, prop = 0.8) -->

<!-- asec_pca_train <- remove_val_labels(training(split)) -->
<!-- asec_pca_test <- remove_val_labels(testing(split)) -->

<!-- # Set up 4 v-folds -->
<!-- folds_pca <- vfold_cv(data = asec_pca_train, v = 4) -->

<!-- # Create recipe -->
<!-- asec_pca_rec <- -->
<!--   recipe(employed ~ ., data = asec_pca_train) %>% -->
<!--   step_center(all_numeric_predictors()) %>% # center predictors -->
<!--   step_scale(all_numeric_predictors()) %>% # scale predictors -->
<!--   step_nzv(all_numeric_predictors()) %>%   # drop near zero variance predictors -->
<!--   step_pca(all_numeric(), num_comp = 20) %>% -->
<!--   themis::step_downsample(employed) %>% # subsampling due to class imbalances between employment class -->
<!--   step_other() -->

<!-- # See the engineered training data -->
<!-- bake(prep(asec_pca_rec, training = asec_pca_train), new_data = asec_pca_train) -->

<!-- # Build the model -->
<!-- logistic_pca_mod <- logistic_reg(penalty = 1) %>% -->
<!--   set_engine("glmnet") -->

<!-- # Create a workflow -->
<!-- logistic_pca_workflow <- -->
<!--   workflow() %>% -->
<!--   add_model(logistic_pca_mod) %>% -->
<!--   add_recipe(asec_pca_rec) -->

<!-- # Fit to folds -->
<!-- logistic_pca_cv <- logistic_pca_workflow %>% -->
<!--   fit_resamples(resamples = folds_pca) -->

<!-- # Calculate RMSE and MAE for each fold -->
<!-- collect_metrics(logistic_pca_cv, summarize = FALSE) -->

<!-- # Select best model based on roc_auc -->
<!-- logistic_pca_best <- logistic_pca_cv %>% -->
<!--   select_best(metric = "roc_auc") -->

<!-- # Finalize workflow with best model -->
<!-- logistic_last_pca_workflow <- logistic_pca_workflow %>% -->
<!--   finalize_workflow(parameters = logistic_pca_best) -->

<!-- # Fit to the all training data and check feature importance -->
<!-- set.seed(20220428) #Setting seed because Marlyn worries about reproducibility -->

<!-- logistic_last_fit <- logistic_last_pca_workflow %>% -->
<!--   fit(data = asec_pca_train) -->

<!-- # Apply model to testing data -->
<!-- log_reg_predictions <- bind_cols(asec_pca_test, -->
<!--                             predict(object = logistic_last_fit, new_data = asec_pca_test), -->
<!--                             predict(object = logistic_last_fit, new_data = asec_pca_test, type = "prob")) -->

<!-- ``` -->

<!-- Looking at the results of the logistic regression below, we see that our model predicts exclusively zeros, meaning that it performs no better than a random guess as to a person's unemployment status (see area under the curve as 0.5). -->
<!-- ```{r log-assessment} -->
<!-- # Assess its performance -->
<!-- conf_mat(data = log_reg_predictions, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # How often the model is correct (overall) -->
<!-- accuracy(data = log_reg_predictions, -->
<!--           truth = employed, -->
<!--           estimate = .pred_class) -->

<!-- # How often the model is correct when a person is actually unemployed -->
<!-- spec(data = log_reg_predictions, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # Our ROC_AUC metric, which also serves as our out-of-sample error rate -->
<!-- roc_auc(data = log_reg_predictions, -->
<!--      truth = employed, -->
<!--      estimate = .pred_0) -->
<!-- ``` -->

<!-- Because were not content with the performance of our final logistic regression model, we decided to compare it to a random forest algorithm. We wanted to tune for the minimum number of data points in a node that are required for a random forest node to be split further (min_n). While we also wanted to tune for number of predictors randomly sampled at each split within individual trees (mtry), we came to realize that it was a very computationally expensive process and limited tuning to only one hyperparameter. -->

<!-- ```{r run-rf-model, results="hide"} -->
<!-- # Preparing data for models -->
<!-- asec_models_2019_2020 <- asec_2019_2020 %>% -->
<!--   filter(year == 2019 | year == 2020) %>% -->
<!--   filter(!is.na(employed)) %>% -->
<!--   mutate(employed = as.factor(employed)) %>% # Make our y variable a factor -->
<!--   select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include as predictors -->
<!--   select(-region, -county, -metro, -metarea, -metfips) %>% # deselect most location variables other than county -->
<!--   select(-empstat, -labforce) %>% # deselect variables that are unuseful (labforce) -->
<!--   mutate_at(vars(race, unitsstr, citizen, hispan, -->
<!--                  occ, ind, educ, classwly, -->
<!--                  strechlk, spmmort, whymove, health, paidgh, statefip), list(~ as.factor(.))) -->

<!-- # Set seed so that selection of training/testing data is consistent between runs -->
<!-- # of the code chunk -->
<!-- set.seed(20201020) -->

<!-- # Split into training and testing data -->
<!-- split <- rsample::initial_split(data = asec_models_2019_2020, prop = 0.8) -->

<!-- asec_train <- remove_val_labels(training(split)) -->
<!-- asec_test <- remove_val_labels(testing(split)) -->

<!-- # Set up 10 v-folds -->
<!-- folds <- vfold_cv(data = asec_train, v = 10) -->

<!-- # Create rf recipe -->
<!-- rf_rec <- -->
<!--   recipe(employed ~ ., data = asec_train) %>% -->
<!--   step_dummy(race, unitsstr, citizen, hispan, educ, -->
<!--              classwly, strechlk, spmmort, whymove, -->
<!--              health, paidgh, statefip) %>% #Dummy select categorical variables -->
<!--   step_center(all_numeric_predictors()) %>% # center predictors -->
<!--   step_scale(all_numeric_predictors()) %>% # scale predictors -->
<!--   step_nzv(all_numeric_predictors()) %>%   # drop near zero variance predictors -->
<!--   themis::step_downsample(employed) %>% # subsampling due to class imbalances between employment class -->
<!--   step_other() -->

<!-- # Build a random forest model (hyperparameter tuning for no. of trees and predictors sampled at each split) -->
<!-- rf_mod <- rand_forest(mtry = 10, min_n = tune(), trees = 100) %>% -->
<!--   set_engine("ranger", importance = "impurity") %>% -->
<!--   set_mode("classification") -->

<!-- # Create a workflow -->
<!-- rf_workflow <- -->
<!--   workflow() %>% -->
<!--   add_model(rf_mod) %>% -->
<!--   add_recipe(rf_rec) -->

<!-- # Create a grid of the parameters we're tuning for -->
<!-- rf_grid <- grid_regular( -->
<!--   min_n(range = c(2, 8)), -->
<!--   levels = 4) -->

<!-- # Execute hyperparameter tuning using the grid and the cross_validation folds -->
<!-- rf_cv <- tune_grid(rf_workflow, -->
<!--                    resamples = folds, -->
<!--                    grid = rf_grid, -->
<!--                    metrics = metric_set(roc_auc)) -->

<!-- #Calculate ROC_AUC and accuracy for each fold -->
<!-- collect_metrics(rf_cv, summarize = TRUE) %>% -->
<!--   filter(.metric == "roc_auc") -->

<!-- # Select best model based on roc_auc -->
<!-- rf_best <- rf_cv %>% -->
<!--   select_best(metric = "roc_auc") -->

<!-- # Finalize model -->
<!-- rf_final_model <- finalize_model(rf_mod, rf_best) -->

<!-- # Finalize workflow (2nd way) -->
<!-- rf_last_workflow <- workflow() %>% -->
<!--   add_recipe(rf_rec) %>% -->
<!--   add_model(rf_final_model) -->

<!-- # Fit to the all training data -->
<!-- set.seed(20220429) #Setting seed -->
<!-- rf_last_fit <- rf_last_workflow %>% -->
<!--   fit(asec_train) -->

<!-- ``` -->


<!-- ```{r rf-feat-imp} -->
<!-- # Look at feature importance -->
<!-- rf_last_fit %>% -->
<!--   extract_fit_parsnip() %>% -->
<!--   vip::vip(num_features = 20) -->

<!-- ``` -->

<!-- ``` {r rf-model-to-test, results="hide"} -->
<!-- # Apply model to testing data -->
<!-- rf_predictions <- bind_cols(asec_test, -->
<!--                             predict(object = rf_last_fit, new_data = asec_test), -->
<!--                             predict(object = rf_last_fit, new_data = asec_test, type = "prob")) -->

<!-- ``` -->

<!-- ``` {r eval-rf-model} -->
<!-- # Assess its performance -->
<!-- conf_mat(data = rf_predictions, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # How often the model is correct (overall) -->
<!-- accuracy(data = rf_predictions, -->
<!--           truth = employed, -->
<!--           estimate = .pred_class) -->

<!-- # How often the model is correct when a person is actually unemployed -->
<!-- spec(data = rf_predictions, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # Our ROC_AUC metric, which also serves as our out-of-sample error rate -->
<!-- roc_auc(data = rf_predictions, -->
<!--      truth = employed, -->
<!--      estimate = .pred_0) -->

<!-- ``` -->

<!-- The area under the curve for our best RF model was 0.75, which is acceptable but not _superb_. We also are interested in the specificity of our model because policy practitioners looking to target unemployed folks for unemployment assistance or workforce opportunities are likely going to want to know how often our model is right when it predicts unemployment. Our model is correct 75% of the time it predicts unemployment. Ideally, we would want better precision so that we could be assured that any money spent on outreach would be well-invested. -->

<!-- Now equipped with the best model, we wanted to see if it would also make precise predictions for new data! We used 2021 IPUMS data that had been excluded from our training set. Essentially, we wanted to assess whether our model that was trained on 2019 and 2020 data could make as good predictions for 2021 data. -->

<!-- ``` {r run-model-2021-data, results="hide"} -->

<!-- # Prepare our implementation data -->
<!-- asec2021_models <- as_tibble(asec2021) %>% -->
<!--   filter(!is.na(employed)) %>% -->
<!--   mutate(employed = as.factor(employed)) %>% # Make our y variable a factor -->
<!--   select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include as predictors -->
<!--   select(-region, -county, -metro, -metarea, -metfips) %>% # deselect most location variables other than county -->
<!--   select(-empstat, -labforce) %>% # deselect variables that are unuseful (labforce) -->
<!--   mutate_at(vars(race, unitsstr, citizen, hispan, -->
<!--                  occ, ind, educ, classwly, -->
<!--                  strechlk, spmmort, whymove, health, paidgh, statefip), list(~ as.factor(.))) -->

<!-- # Apply model to 2021 data -->
<!-- rf_predictions_2021 <- bind_cols(asec2021_models, -->
<!--                             predict(object = rf_last_fit, new_data = asec2021_models), -->
<!--                             predict(object = rf_last_fit, new_data = asec2021_models, type = "prob")) -->
<!-- ``` -->

<!-- ``` {r eval-model-2021-data} -->
<!-- # Assess its performance -->
<!-- conf_mat(data = rf_predictions_2021, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # How often the model is correct (overall) -->
<!-- accuracy(data = rf_predictions_2021, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # How often the model is correct when a person is actually unemployed -->
<!-- spec(data = rf_predictions_2021, -->
<!--      truth = employed, -->
<!--      estimate = .pred_class) -->

<!-- # Our ROC_AUC metric, which also serves as our out-of-sample error rate -->
<!-- roc_auc(data = rf_predictions_2021, -->
<!--         truth = employed, -->
<!--         estimate = .pred_0) -->

<!-- ``` -->

<!-- ##How well does our model perform if applied to specific demographic groups? -->

<!-- While our model fared well enough when we made predictions on IPUMS 2021 data [EDIT IF NOT], we wanted to see if the model would perform just as well if we were to restrict the population we wanted to predict employment status for. We decided to make predictions on a sample of  immigrants (excluding US native-born citizens) because we understood immigrants experienced the onset of COVID-19 in nuanced ways. For example, the below graph illustrates surprising find: while it's well-documented that the US unemployment rate rose during the course of the pandemic, the unemployment rate of US nativeborn citizens surpassed the rate for immigrants. -->


<!-- ``` {r Immigrant-Specific-Visualization} -->

<!-- # percentage point change of unemployed people by immigrant status -->
<!-- a <- asec_allyears %>% -->
<!--   filter(!is.na(employed))%>% -->
<!--   #filter(year ==2019 | year == 2020) %>% #Including 2021 so we can visualize how unemployment rose for both immigrants and nonimmigrants -->
<!--   select(year, employed, immigrant) %>% -->
<!--   group_by(year, immigrant) %>% -->
<!--   mutate(immigrant = as.factor(immigrant)) %>% -->
<!--   summarize(count = n(), -->
<!--             pct_unemployed = (1-mean(employed))) -->

<!-- a %>% -->
<!--   ggplot(aes(x= year, y= pct_unemployed)) + -->
<!--   geom_line(aes(color = immigrant, group = immigrant)) + -->
<!--   geom_point(size = 1) + -->
<!--    labs( -->
<!--     title = "Unemployment Rate Rose in the US During the Pandemic", -->
<!--     caption = "Source: IPUMS Data", -->
<!--     x = "", -->
<!--     y = "Unemployment Rate", -->
<!--     color = "Immigrant Status" -->
<!--   ) + -->
<!--   scale_y_continuous(labels = scales::percent, limits = c(0,.10)) + -->
<!--   scale_x_continuous(breaks = seq(2019, 2021, 1), -->
<!--                      limits=c(2019, 2021)) + -->
<!--   scale_color_discrete(labels = c("Nonimmigrant", "Immigrant")) + -->
<!--   theme_minimal() -->

<!-- ``` -->

<!-- While we understood the unlikelihood that our model trained on the entire US population would perform well on a specific subgroup, we wanted to test it out as data science curious novices. Would the model still fare better than random guessing (indicative when the ROC area under the curve is 0.5)? -->

<!-- ``` {r run-model-immigrant-data, results="hide"} -->

<!-- # Create data frame only of immigrants -->
<!-- asec_model_imm <- as_tibble(asec2021_imm) %>% -->
<!--   filter(!is.na(employed)) %>% -->
<!--   mutate(employed = as.factor(employed)) %>% # Make our y variable a factor -->
<!--   select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include as predictors -->
<!--   select(-region, -county, -metro, -metarea, -metfips) %>% # deselect most location variables other than county -->
<!--   select(-empstat, -labforce) %>% # deselect variables that are unuseful (labforce) -->
<!--   mutate_at(vars(race, unitsstr, citizen, hispan, -->
<!--                  occ, ind, educ, classwly, -->
<!--                  strechlk, spmmort, whymove, health, paidgh, statefip), list(~ as.factor(.))) -->

<!-- # Apply model to 2021 immigrant data -->
<!-- rf_predictions_imm <- bind_cols(asec_model_imm, -->
<!--                                  predict(object = rf_last_fit, new_data = asec_model_imm), -->
<!--                                  predict(object = rf_last_fit, new_data = asec_model_imm, type = "prob")) -->
<!-- ``` -->

<!-- ``` {r eval-model-immigrant-data} -->
<!-- # Assess its performance -->
<!-- conf_mat(data = rf_predictions_imm, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # How often the model is correct (overall) -->
<!-- accuracy(data = rf_predictions_imm, -->
<!--          truth = employed, -->
<!--          estimate = .pred_class) -->

<!-- # How often the model is correct when a person is actually unemployed -->
<!-- spec(data = rf_predictions_imm, -->
<!--      truth = employed, -->
<!--      estimate = .pred_class) -->

<!-- # Our ROC_AUC metric, which also serves as our out-of-sample error rate -->
<!-- roc_auc(data = rf_predictions_imm, -->
<!--         truth = employed, -->
<!--         estimate = .pred_0) -->

<!-- ``` -->

<!-- [HERE IS WHERE WE TALK ABOUT WHAT WE FOUND - DID MODEL FARE WELL APPLYING TO IMMIGRANT SUBGROUP?] -->