---
title: "Final_Project"
author: "Sylvia Brown, Pierina Forastieri, Julia Buschmann, Marlyn Bruno"
date: "2022-05-06"
output: 
  html_document:
    code_folding: "hide"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

``` {r add-libraries}
library(formattable)
library(ggalt)
library(haven)
library(ipumsr)
library(labelled)
library(patchwork)
library(sjlabelled)
library(srvyr)
library(tidyverse)

```

How to customize .Rmd:
- To suppress output from the code chunk, include `results = "hide"` in the title for the code chunk
- To center the figure, include `fig.align = "center"` in the title for the code chunk

```{r loading data}
ddi <- read_ipums_ddi("data/cps_00004.xml")
data <- read_ipums_micro(ddi) %>%
  janitor::clean_names()

cps_svy <- data %>%
  filter(empstat != 1) %>% #filter  out people in armed forces
  filter(age >= 18 & age < 66) %>% #filter out minors; leave only adults
  mutate(employed = case_when(
    empstat == 10 | empstat == 12 ~ 1,
    empstat == 21 | empstat == 22 ~ 0,
    TRUE ~ NA_real_
  )) %>%
  mutate_at(vars(offpov, himcarenw, caidnw, anycovly, prvtcovnw, grpcovnw, mrkcovnw, mrkscovnw, mrkucovnw, inhcovnw, sex), list(~ case_when(
    . == 2 ~ 1,
    . == 1 ~ 0,
    TRUE ~ NA_real_
  ))) %>%
  mutate(unitsstr = case_when(
    unitsstr == 00 ~ NA_real_,
    TRUE ~ as.numeric(unitsstr)
  )) %>%
  mutate(region = case_when(
    region == 97 ~ NA_real_,
    TRUE ~ as.numeric(region)
  )) %>%
  filter(statefip <= 56) %>% # drop weird multi-state fips codes
  mutate(metro = case_when(
    metro == 0 | metro >= 4 ~ NA_real_,
    TRUE ~ as.numeric(metro)
  )) %>%
  mutate(metarea = case_when(
    metarea >= 9997 ~ NA_real_,
    TRUE ~ as.numeric(metarea)
  )) %>%
  mutate(metfips = case_when(
    metfips >= 99998 ~ NA_real_,
    TRUE ~ as.numeric(metfips)
  )) %>%
  mutate(hhincome = case_when(
    hhincome == 9999999 ~ NA_real_,
    TRUE ~ as.numeric(hhincome)
  )) %>%
  mutate_at(vars(county, empstat, labforce, disabwrk), list(~ case_when(
    . == 0 ~ NA_real_,
    TRUE ~ as.numeric(.)
  ))) %>%
    mutate(educ = case_when(
    educ == 001 | educ == 999 ~ NA_real_,
    TRUE ~ as.numeric(educ)
  )) %>%
  mutate(ftotval = case_when(
    ftotval == 9999999999 ~ NA_real_,
    TRUE ~ as.numeric(ftotval)
  )) %>%
  mutate(inctot = case_when(
    inctot == 999999999 ~ NA_real_,
    TRUE ~ as.numeric(inctot)
  )) %>%
  mutate(incwelfr = case_when(
    incwelfr == 999999 ~ NA_real_,
    TRUE ~ incwelfr
  )) %>%
  mutate(incunemp = case_when(
    incunemp == 999999 ~ NA_real_,
    TRUE ~ as.numeric(incunemp)
  )) %>%
  mutate(ctccrd = case_when(
    ctccrd == 999999 ~ NA_real_,
    TRUE ~ as.numeric(ctccrd)
  )) %>%
  mutate(eitcred = case_when(
    eitcred == 9999 ~ NA_real_,
    TRUE ~ as.numeric(eitcred)
  )) %>%
  mutate(immigrant = case_when(
    citizen == 1 ~ 1,
    citizen >= 2 & citizen <= 5 ~ 0,
    TRUE ~ NA_real_
  )) %>%
    mutate(wksunem1 = case_when(
    wksunem1 == 99 ~ 0,
    TRUE ~ as.numeric(wksunem1)
  )) %>%
    mutate(wksunem2 = case_when(
    wksunem2 == 9 ~ 0,
    TRUE ~ as.numeric(wksunem2)
  )) %>%
  select(-starts_with("q"), -asecwt, -month, -asecflag, -statecensus, -asecwth, -pernum, -cpsidp) # Drop quality flags and variables we don't need
  # NOTE: the 'immigrant' variable is 1 for people born in the U.S.; 0 for people born in the U.S. outlying territories (PR, Guam), people born abroad of American parents, naturalized citizens, and non-citizens; and NA for people marked as NIU.

asec_allyears <- as_survey_design(cps_svy, weights = asecwtcvd)

asec2019 <- asec_allyears %>%
  filter(year == 2019)

asec2020 <- asec_allyears %>%
  filter(year == 2020)

asec2021 <- asec_allyears %>%
  filter(year == 2021)

asec_allyears_imm <- asec_allyears %>%
  filter(immigrant == 1)

asec2019_imm <- asec2019 %>%
  filter(immigrant == 1)

asec2020_imm <- asec2020 %>%
  filter(immigrant == 1)

asec2021_imm <- asec2021 %>%
  filter(immigrant == 1)
  
```

```{r EDA}
# % unemployed, employed, out of labor force in each 
asec_allyears %>%
  drop_na(employed) %>%
  select(year, employed) %>%
  group_by(year, employed) %>%
  summarise(pct = survey_prop()*100) %>%
  ggplot(aes(x=year, y=pct, fill=factor(employed))) + 
  geom_col(position="dodge") +
  labs(
    title = "Percentage of Unemployed and Employed Out of the Labor Force",
    subtitle = "(in % for 2019, 2020, 2021)",
    caption = "Source: IPUMS Data",
    x = "Year",
    y = "%",
    fill = "Status",
  ) +
  geom_text(aes(label = round(pct,2)), 
                position = position_dodge(width = 1), 
                color="black",vjust = -.2) +
  scale_fill_hue(labels = c("Unemployed", "Employed")) +
  theme(legend.title = element_blank()) +
  theme_minimal()
 

# How does employment vary by age?
# employed
employed_graph <- asec_allyears %>%
   mutate(
    # Create categories
    age_group = case_when(
      age > 17 & age <= 25 ~ "18-24",
      age > 24 & age <= 31 ~ "25-31",
      age > 21 & age <= 38 ~ "32-38",
      age > 38 & age <= 45 ~ "39-45",
      age > 45 & age <= 52 ~ "46-52",
      age > 52 & age <= 59 ~ "53-59",
      age > 59 & age <= 65 ~ "60-65"
    )) %>%
  drop_na(employed) %>%
  filter(employed == 1) %>%
  select(year, employed, age, age_group) %>%
  group_by(year, age_group) %>%
  summarise(cnt = n()) %>%
  mutate(freq = percent(cnt / sum(cnt))) %>%
  ggplot(aes(x=age_group, y=freq, fill=factor(year))) + 
  geom_col(position="dodge") +
  geom_text(aes(age_group, label = freq), position = position_dodge(width = 1), hjust = -.25) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent, limits = c(0,.3)) +
  labs(
    title = "Employed by Age Group",
    subtitle = "(for 2019, 2020, 2021)",
    caption = "Source: IPUMS Data",
    x = "Age Group",
    y = "",
    fill = "Year",
  ) +
  theme(legend.title = element_blank()) +
  theme_minimal()

# unemployed
unemployed_graph <- asec_allyears %>%
   mutate(
    # Create categories (18-24; 24-30; 31-37; 38-44, 45-51, 52-58, 58-64)
    age_group = case_when(
      age > 17 & age <= 25 ~ "18-24",
      age > 24 & age <= 31 ~ "25-31",
      age > 21 & age <= 38 ~ "32-38",
      age > 38 & age <= 45 ~ "39-45",
      age > 45 & age <= 52 ~ "46-52",
      age > 52 & age <= 59 ~ "53-59",
      age > 59 & age <= 65 ~ "60-65"
    )) %>%
  drop_na(employed) %>%
  filter(employed == 0) %>%
  select(year, employed, age, age_group) %>%
  group_by(year, age_group) %>%
  summarise(cnt = n()) %>%
  mutate(freq = percent(cnt / sum(cnt))) %>%
  ggplot(aes(x=age_group, y=freq, fill=factor(year))) + 
  geom_col(position="dodge") +
  geom_text(aes(age_group, label = freq), position = position_dodge(width = 1), hjust = -.25) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent, limits = c(0,.35)) +
  labs(
    title = "Unemployed by Age Group",
    subtitle = "(in % for 2019, 2020, 2021)",
    caption = "Source: IPUMS Data",
    x = "Age Group",
    y = "",
    fill = "Status",
  ) +
  theme(legend.title = element_blank()) +
  theme_minimal()

employed_graph + unemployed_graph

# dumbbell % unemployed by state between 2019 and 2020
status_bystate <- asec_allyears %>%
  drop_na(employed) %>%
  filter(year ==2019 | year == 2020) %>%
  select(year, employed, statefip) %>%
  group_by(year, statefip, employed) %>% 
  summarise(cnt = n()) %>%
  mutate(freq = percent(cnt / sum(cnt))) 

status_bystate %>%
  filter(employed == 0) %>%
  select(year, statefip, freq) %>%
  pivot_wider(names_from = year, values_from = freq) %>%
  ggplot(aes(x=`2019`, xend=`2020`, y=as_label(statefip))) +
  geom_dumbbell(color="#a3c4dc", 
                colour_x="#edae52", 
                      colour_xend = "#9fb059",
                      size=0.75, 
                      point.colour.l="#0e668b") + 
  scale_x_continuous(label = percent)  +
  labs(
    title = "Unemployed by State",
    subtitle = "(2019 'yellow' and 2020 'green')",
    caption = "Source: IPUMS Data",
    x = "",
    y = "States",
  ) +
  theme_minimal()

```

```{r model-prep}
# Loading libraries
library(tidymodels)
library(survey)
library(parsnip)

# New modeling script
# Create tibble with 2019 and 2020 data for model training, testing
# Removes variable we're trying to predict (employed)
asec_2019_2020 <- cps_svy %>%
  filter(year == 2019 | year == 2020) %>%
  filter(!is.na(employed))

```

When conceptualizing the potential different models we could build to predict people's employment status, we were curious to see if principal components could be used as predictors. To avoid data leakage, it is necessary for the principal components to be constructed each time our model specifications run through new cross-validation folds. As such, principal component analysis needed to be part of our preprocessing, and we included it as the last step of our model recipe. Nevertheless, we had to construct a specific data set to use principal components as predictors since we needed to convert categorical variables into dummy variables without increasing their weight in the machine learning algorithms. 

```{r prep-data-pca-modeling}
# ------------------------------------Creating data frame for step_pca------------------------------------

# Kept yrimmig and strechlk with 0 variable for NIU
# Recoded NIU values for wksunem1 (99) and wksunem2  (9) to zero

# Select relevant variables from data set
asec_pca_2019_2020 <- asec_2019_2020 %>%
  select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include in PCA analysis
  select(-region, -county, -metro, -metarea, -metfips) %>% # deselect all location variables other than state
  select(-empstat, -labforce, -asecwtcvd) %>% # deselect variables that are unuseful (labforce) or redundant with employed variable
  mutate_at(vars(race, unitsstr, citizen, hispan,
            occ, ind, educ, classwly,
            strechlk, spmmort, health, paidgh, whymove, statefip, employed), list(~ as.factor(.)))

asec_pca_2019_2020 <- recipe(~ ., asec_pca_2019_2020) %>%
  step_dummy(race, unitsstr, citizen, hispan,
             occ, ind, educ, classwly,
             strechlk, spmmort, whymove, health, paidgh, statefip) %>%
  prep() %>%
  bake(asec_pca_2019_2020)
# Note: did not include in step_dummy: hhincome, age, yrimmig, existing indicator variables (sex,
# offpov, disabwrk, himcarenw, caidnw, anycovly, prvtcovnw, grpcovnw, mrkcovnw, 
# mrkscovnw, inhcovnw, schipnw), wksunem1, wksunem2, ftotval, inctto, incwelfr, 
# incunemp, ctccrd, eitcred, moop, hipval

asec_pca_2019_2020 <- asec_pca_2019_2020 %>%
  mutate_at(
    vars(
      offpov, himcarenw, caidnw, anycovly, prvtcovnw, grpcovnw, mrkcovnw,
      mrkscovnw, inhcovnw, mrkucovnw, sex, starts_with("spmmort")
    ),
    list( ~ case_when(. == 1 ~ (1/sqrt(2)),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("citizen"), starts_with("health")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(4),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("unitsstr")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(5),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("classwly")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(7),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("hispan")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(8),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("educ")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(15),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("race")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(25),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("state")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(50),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_)) 
    ) %>%
  mutate_at(
    vars(
      starts_with("ind")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(279),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("occ")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(633),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("classwly")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(7),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("whymove")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(20),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  ) %>%
  mutate_at(
    vars(
      starts_with("paidgh")
    ),
    list( ~ case_when(. == 1 ~ 1/sqrt(3),
                      . == 0 ~ 0,
                      TRUE ~ NA_real_))
  )

# asec_pca_2019_2020 can be run by the models and apply PCA to them (using step_pca)

```

We decided to first try using principal components as predictors in a logistic regression model. 

``` {r Model 1}
# Preparing data for models
#USE asec_pca_2019_2020

# Set seed so that selection of training/testing data is consistent between runs
# of the code chunk
set.seed(20201020)

# Split into training and testing data
split <- initial_split(data = asec_pca_2019_2020, prop = 0.8)

asec_pca_train <- remove_val_labels(training(split))
asec_pca_test <- remove_val_labels(testing(split))

# Set up 10 v-folds
folds_pca <- vfold_cv(data = asec_pca_train, v = 10)

# Create recipe
asec_pca_rec <-
  recipe(employed ~ ., data = asec_pca_train) %>%
  step_center(all_numeric_predictors()) %>% # center predictors
  step_scale(all_numeric_predictors()) %>% # scale predictors
  step_nzv(all_numeric_predictors()) %>%   # drop near zero variance predictors
  step_pca(all_numeric(), num_comp = 20) %>%
  themis::step_downsample(employed) %>% # subsampling due to class imbalances between employment class 
  step_other() 

# See the engineered training data
bake(prep(asec_pca_rec, training = asec_pca_train), new_data = asec_pca_train)

# Build the model
logistic_pca_mod <- logistic_reg(penalty = 1) %>% 
  set_engine("glmnet")

# Create a workflow
logistic_pca_workflow <- 
  workflow() %>% 
  add_model(logistic_pca_mod) %>% 
  add_recipe(asec_pca_rec)

# Fit to folds
logistic_pca_cv <- logistic_pca_workflow %>% 
  fit_resamples(resamples = folds_pca)

# Calculate RMSE and MAE for each fold 
collect_metrics(logistic_pca_cv, summarize = FALSE) 

# Select best model based on roc_auc
logistic_pca_best <- logistic_pca_cv %>%
  select_best(metric = "roc_auc")

# Finalize workflow with best model
logistic_last_pca_workflow <- logistic_pca_workflow %>%
  finalize_workflow(parameters = logistic_pca_best)

# Fit to the all training data and check feature importance
set.seed(20220428) #Setting seed because Marlyn worries about reproducibility 

logistic_last_fit <- logistic_last_pca_workflow %>%
  fit(data = asec_pca_train)

predict(logistic_last_fit, asec_pca_train) %>% 
  group_by(.pred_class) %>% 
  summarize(n = n())

```

[TALK ABOUT HOW THE PCA-LM MODEL DID HERE]...We were not content with the performance of our final logistic regression model. We decided to next try a Random Forest algorithm. We wanted to tune for the minimum number of data points in a node that are required for a Random Forest node to be split further (min_n). While we also wanted to tune for number of predictors randomly sampled at each split within individual trees (mtry), we came to realize that it was a very computationally expensive process and limited tuning to only one hyperparameter. 

```{r Model 2}
# Preparing data for models
asec_models_2019_2020 <- asec_2019_2020 %>%
  filter(year == 2019 | year == 2020) %>%
  filter(!is.na(employed)) %>%
  mutate(employed = as.factor(employed)) %>% # Make our y variable a factor
  select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include as predictors
  select(-region, -county, -metro, -metarea, -metfips) %>% # deselect most location variables other than county
  select(-empstat, -labforce) %>% # deselect variables that are unuseful (labforce)
  mutate_at(vars(race, unitsstr, citizen, hispan,
                 occ, ind, educ, classwly,
                 strechlk, spmmort, whymove, health, paidgh, statefip), list(~ as.factor(.)))

# Set seed so that selection of training/testing data is consistent between runs
# of the code chunk
set.seed(20201020)

# Split into training and testing data
split <- initial_split(data = asec_models_2019_2020, prop = 0.8)

asec_train <- remove_val_labels(training(split))
asec_test <- remove_val_labels(testing(split))

# Set up 10 v-folds
folds <- vfold_cv(data = asec_train, v = 10)

# Create rf recipe
rf_rec <-
  recipe(employed ~ ., data = asec_train) %>%
  step_dummy(race, unitsstr, citizen, hispan, educ, 
             classwly, strechlk, spmmort, whymove, 
             health, paidgh, statefip) %>% #Dummy select categorical variables
  step_center(all_numeric_predictors()) %>% # center predictors
  step_scale(all_numeric_predictors()) %>% # scale predictors
  step_nzv(all_numeric_predictors()) %>%   # drop near zero variance predictors
  themis::step_downsample(employed) %>% # subsampling due to class imbalances between employment class 
  step_other()

# Build a random forest model (hyperparameter tuning for no. of trees and predictors sampled at each split)
rf_mod <- rand_forest(mtry = 10, min_n = tune(), trees = 100) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Create a workflow
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_rec)

# Create a grid of the parameters we're tuning for
rf_grid <- grid_regular(
  min_n(range = c(2, 8)),
  levels = 4)

# Execute hyperparameter tuning using the grid and the cross_validation folds
rf_cv <- tune_grid(rf_workflow,
                   resamples = folds,
                   grid = rf_grid,
                   metrics = metric_set(roc_auc))

#Calculate ROC_AUC and accuracy for each fold 
collect_metrics(rf_cv, summarize = TRUE) %>%
  filter(.metric == "roc_auc")

# Select best model based on roc_auc
rf_best <- rf_cv %>%
  select_best(metric = "roc_auc")

# Finalize model
rf_final_model <- finalize_model(rf_mod, rf_best)

# Finalize workflow (2nd way)
rf_last_workflow <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_final_model)

# Fit to the all training data
set.seed(20220429) #Setting seed because Marlyn worries about reproducibility 
rf_last_fit <- rf_last_workflow %>%
  fit(asec_train)

# Look at feature importance
rf_last_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20)

# Apply model to testing data
rf_predictions <- bind_cols(asec_test, 
                            predict(object = rf_last_fit, new_data = asec_test),
                            predict(object = rf_last_fit, new_data = asec_test, type = "prob"))

# Assess its performance
conf_mat(data = rf_predictions,
         truth = employed,
         estimate = .pred_class)

# How often the model is correct (overall)
accuracy(data = rf_predictions,
          truth = employed,
          estimate = .pred_class)

# How often the model is correct when a person is actually unemployed
spec(data = rf_predictions,
         truth = employed,
         estimate = .pred_class)

# Our ROC_AUC metric, which also serves as our out-of-sample error rate
roc_auc(data = rf_predictions,
     truth = employed,
     estimate = .pred_0)

```

The area under the curve for our best RF model was 0.75, which is acceptable but not _superb_. We also are interested in the specificity of our model because policy practitioners looking to target unemployed folks for unemployment assistance or workforce opportunities are likely going to want to know how often our model is right when it predicts unemployment. Our model is correct 75% of the time it predicts unemployment. Ideally, we would want better precision so that we could be assured that any money spent on outreach would be well-invested.

We also attempted to run the random forest with the principal components as predictors, but couldn't get it to run. [SYLIVA, delete this if you don't go ahead with PC-RF attempt]

```{r model-random-forest-pca}
# # Build a random forest that will incorporate principal components as predictors
# rf_mod_pca <- rand_forest(mtry = 10, min_n = tune(), trees = 100) %>%
#   set_engine("ranger", importance = "impurity") %>%
#   set_mode("classification")
# 
# # Create a workflow
# rf_workflow_pca <- 
#   workflow() %>% 
#   add_model(rf_mod_pca) %>% 
#   add_recipe(asec_pca_rec)
# 
# # Create a grid of the parameters we're tuning for
# rf_grid_pca <- grid_regular(
#   min_n(range = c(2, 8)),
#   levels = 4)
# 
# # Execute hyperparameter tuning using the grid and the cross_validation folds
# rf_cv_pca <- tune_grid(rf_workflow_pca,
#                    resamples = folds_pca,
#                    grid = rf_grid_pca,
#                    metrics = metric_set(roc_auc))

```

Now equipped with the best model, we wanted to see if it would also make precise predictions for new data! We used 2021 IPUMS data that had been excluded from our training set. Essentially, we wanted to assess whether our model that was trained on 2019 and 2020 data could make as good predictions for 2021 data. 

``` {r run-model-immigrant-data}

# Prepare our implementation data
asec2021_models <- asec2021 %>%
  filter(!is.na(employed)) %>%
  mutate(employed = as.factor(employed)) %>% # Make our y variable a factor
  select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include as predictors
  select(-region, -county, -metro, -metarea, -metfips) %>% # deselect most location variables other than county
  select(-empstat, -labforce) %>% # deselect variables that are unuseful (labforce)
  mutate_at(vars(race, unitsstr, citizen, hispan,
                 occ, ind, educ, classwly,
                 strechlk, spmmort, whymove, health, paidgh, statefip, employed), list(~ as.factor(.)))

# Apply model to 2021 data
rf_predictions_2021 <- bind_cols(asec2021_models, 
                            predict(object = rf_last_fit, new_data = asec2021_models),
                            predict(object = rf_last_fit, new_data = asec2021_models, type = "prob"))

# Assess its performance
conf_mat(data = rf_predictions_2021,
         truth = employed,
         estimate = .pred_class)

# How often the model is correct (overall)
accuracy(data = rf_predictions_2021,
         truth = employed,
         estimate = .pred_class)

# How often the model is correct when a person is actually unemployed
spec(data = rf_predictions_2021,
     truth = employed,
     estimate = .pred_class)

# Our ROC_AUC metric, which also serves as our out-of-sample error rate
roc_auc(data = rf_predictions_2021,
        truth = employed,
        estimate = .pred_0)

```

##How well does our model perform if applied to specific demographic groups?

While our model fared well enough when we made predictions on IPUMS 2021 data [EDIT IF NOT], we wanted to see if the model would perform just as well if we were to restrict the population we wanted to predict employment status for. We decided to make predictions on a sample of  immigrants (excluding US native-born citizens) because we understood immigrants experienced the onset of COVID-19 in nuanced ways. For example, the below graph illustrates surprising find: while it's well-documented that the US unemployment rate rose during the course of the pandemic, the unemployment rate of US nativeborn citizens surpassed the rate for immigrants. 


``` {r Immigrant-Specific Visualization}

# percentage point change of unemployed people by immigrant status
a <- asec_allyears %>%
  filter(!is.na(employed))%>%
  #filter(year ==2019 | year == 2020) %>% #Including 2021 so we can visualize how unemployment rose for both immigrants and nonimmigrants
  select(year, employed, immigrant) %>%
  group_by(year, immigrant) %>%
  mutate(immigrant = as.factor(immigrant)) %>%
  summarize(count = n(),
            pct_unemployed = (1-mean(employed))) 

a %>%
  ggplot(aes(x= year, y= pct_unemployed)) +
  geom_line(aes(color = immigrant, group = immigrant)) +
  geom_point(size = 1) +
   labs(
    title = "Unemployment Rate Rose in the US During the Pandemic",
    caption = "Source: IPUMS Data",
    x = "",
    y = "Unemployment Rate",
    color = "Immigrant Status"
  ) +
  scale_y_continuous(labels = scales::percent, limits = c(0,.10)) +
  scale_x_continuous(breaks = seq(2019, 2021, 1), 
                     limits=c(2019, 2021)) +
  scale_color_discrete(labels = c("Nonimmigrant", "Immigrant")) +
  theme_minimal()

```

While we understood the unlikelihood that our model trained on the entire US population would perform well on a specific subgroup, we wanted to test it out as data science curious novices. Would the model still fare better than random guessing (indicative when the ROC area under the curve is 0.5)?

***DO WE WANT TO APPLY TO 2020 OR 2021 IMMIGRANT DATA?

``` {r run-model-immigrant-data}

# Create data frame only of immigrants
asec_model_imm <- as_tibble(asec2021_imm) %>%
  filter(!is.na(employed)) %>%
  mutate(employed = as.factor(employed)) %>% # Make our y variable a factor
  select(-year, -serial, -cpsid, -immigrant) %>% # deselect variables we don't want to include as predictors
  select(-region, -county, -metro, -metarea, -metfips) %>% # deselect most location variables other than county
  select(-empstat, -labforce) %>% # deselect variables that are unuseful (labforce)
  mutate_at(vars(race, unitsstr, citizen, hispan,
                 occ, ind, educ, classwly,
                 strechlk, spmmort, whymove, health, paidgh, statefip, employed), list(~ as.factor(.)))

# Apply model to 2021 immigrant data
rf_predictions_imm <- bind_cols(asec_model_imm, 
                                 predict(object = rf_last_fit, new_data = asec_model_imm),
                                 predict(object = rf_last_fit, new_data = asec_model_imm, type = "prob"))

# Assess its performance
conf_mat(data = rf_predictions_imm,
         truth = employed,
         estimate = .pred_class)

# How often the model is correct (overall)
accuracy(data = rf_predictions_imm,
         truth = employed,
         estimate = .pred_class)

# How often the model is correct when a person is actually unemployed
spec(data = rf_predictions_imm,
     truth = employed,
     estimate = .pred_class)

# Our ROC_AUC metric, which also serves as our out-of-sample error rate
roc_auc(data = rf_predictions_imm,
        truth = employed,
        estimate = .pred_0)

```

[HERE IS WHERE WE TALK ABOUT WHAT WE FOUND - DID MODEL FARE WELL APPLYING TO IMMIGRANT SUBGROUP?]